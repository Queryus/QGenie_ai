# src/core/providers/llm_provider.py

import os
import asyncio
import logging
import time
from typing import Optional
from langchain_openai import ChatOpenAI
from core.clients.api_client import get_api_client

logger = logging.getLogger(__name__)

class LLMProvider:
    """
    LLM ì œê³µìë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    ì§€ì—° ì´ˆê¸°í™”ë¥¼ ì§€ì›í•˜ì—¬ BE ì„œë²„ê°€ ëŠ¦ê²Œ ì‹œì‘ë˜ì–´ë„ ì‘ë™í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0):
        self.model_name = model_name
        self.temperature = temperature
        self._api_client = None
        self._initialization_attempted: bool = False
        self._initialization_failed: bool = False
        # ì§§ì€ ìºì‹±ì„ ìœ„í•œ ë³€ìˆ˜ë“¤ (ì„±ëŠ¥ ìµœì í™”)
        self._cached_api_key: Optional[str] = None
        self._api_key_cache_time: float = 0
        self._api_key_cache_duration: float = 30.0  # 30ì´ˆ ìºì‹±
    
    async def _load_api_key(self) -> str:
        """ë°±ì—”ë“œì—ì„œ OpenAI API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì§§ì€ ì‹œê°„ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”."""
        try:
            current_time = time.time()
            
            # ìºì‹œëœ í‚¤ê°€ ìˆê³  ì•„ì§ ìœ íš¨í•œ ê²½ìš°
            if (self._cached_api_key and 
                current_time - self._api_key_cache_time < self._api_key_cache_duration):
                logger.debug("ğŸ“‹ ìºì‹œëœ API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
                return self._cached_api_key
            
            # ìºì‹œê°€ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ì¡°íšŒ
            if self._api_client is None:
                self._api_client = await get_api_client()
            
            api_key = await self._api_client.get_openai_api_key()
            
            # ìºì‹œ ì—…ë°ì´íŠ¸
            self._cached_api_key = api_key
            self._api_key_cache_time = current_time
            
            logger.debug("ğŸ”„ ë°±ì—”ë“œì—ì„œ ìµœì‹  OpenAI API í‚¤ë¥¼ ì¡°íšŒí•˜ê³  ìºì‹œí–ˆìŠµë‹ˆë‹¤")
            return api_key
            
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ìºì‹œ ë¬´íš¨í™”
            self._cached_api_key = None
            self._api_key_cache_time = 0
            logger.error(f"Failed to fetch API key from backend: {e}")
            raise ValueError("ë°±ì—”ë“œì—ì„œ OpenAI API í‚¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°±ì—”ë“œ ì„œë²„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    async def get_llm(self) -> ChatOpenAI:
        """
        ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        ë§¤ë²ˆ ìµœì‹  API í‚¤ë¡œ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # ì´ì „ì— ì´ˆê¸°í™”ë¥¼ ì‹œë„í–ˆê³  ì‹¤íŒ¨í–ˆë‹¤ë©´ ì¬ì‹œë„
        if self._initialization_failed:
            logger.info("ğŸ”„ LLM ì´ˆê¸°í™” ì¬ì‹œë„ ì¤‘...")
            self._initialization_failed = False
            self._initialization_attempted = False
        
        try:
            self._initialization_attempted = True
            llm = await self._create_llm()
            
            # ì—°ê²° ë³µêµ¬ ê°ì§€
            if self._initialization_failed:
                logger.info("ğŸ‰ LLMProvider: ë°±ì—”ë“œ ì—°ê²°ì´ ë³µêµ¬ë˜ì–´ LLM ì´ˆê¸°í™”ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
            
            self._initialization_failed = False
            logger.debug("âœ… LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ (ìµœì‹  API í‚¤ ì‚¬ìš©)")
            
            return llm
            
        except Exception as e:
            self._initialization_failed = True
            logger.error(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"LLMì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”: {e}")
    
    async def _create_llm(self) -> ChatOpenAI:
        """ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë§¤ë²ˆ ìµœì‹  API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."""
        try:
            # API í‚¤ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë¡œë“œ (ë§¤ë²ˆ ìµœì‹  í‚¤ ì¡°íšŒ)
            api_key = await self._load_api_key()
            logger.debug("âœ… ë°±ì—”ë“œì—ì„œ ìµœì‹  OpenAI API í‚¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤")
            
            llm = ChatOpenAI(
                model=self.model_name,
                temperature=self.temperature,
                api_key=api_key
            )
            return llm
            
        except Exception as e:
            raise RuntimeError(f"LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def update_model(self, model_name: str, temperature: float = None):
        """ëª¨ë¸ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ë‹¤ìŒ get_llm() í˜¸ì¶œì‹œ ìƒˆ ì„¤ì •ì´ ì ìš©ë©ë‹ˆë‹¤."""
        self.model_name = model_name
        if temperature is not None:
            self.temperature = temperature
        logger.info(f"LLM ëª¨ë¸ ì„¤ì • ë³€ê²½: {model_name}, temperature: {temperature}")
    
    async def refresh_api_key(self):
        """API í‚¤ ìºì‹œë¥¼ ë¬´íš¨í™”í•˜ì—¬ ë‹¤ìŒ ìš”ì²­ì—ì„œ ìµœì‹  í‚¤ë¥¼ ì¡°íšŒí•˜ë„ë¡ í•©ë‹ˆë‹¤."""
        self._cached_api_key = None
        self._api_key_cache_time = 0
        self._initialization_attempted = False
        self._initialization_failed = False
        logger.info("ğŸ”„ API í‚¤ ìºì‹œ ë¬´íš¨í™” ì™„ë£Œ (ë‹¤ìŒ ìš”ì²­ë¶€í„° ìµœì‹  í‚¤ ì¡°íšŒ)")
    
    async def test_connection(self) -> bool:
        """LLM ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        try:
            llm = await self.get_llm()
            test_response = await llm.ainvoke("í…ŒìŠ¤íŠ¸")
            return test_response is not None
            
        except Exception as e:
            print(f"LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_llm_provider = LLMProvider()

async def get_llm_provider() -> LLMProvider:
    """LLM Provider ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return _llm_provider
